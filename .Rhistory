i <- 0
j <- 0
k <- 0
#LOOP-1
for(i in c(1:nrow(test_data))){ #looping over each record of test data
eu_dist = c() #eu_dist & eu_char empty vector
eu_char = c()
good = 0 #good & bad variable initialization with 0 value
bad = 0
#LOOP-2-looping over train data
for(j in c(1:nrow(train_data))){
#adding euclidean distance b/w test data point and train data to eu_dist vector
eu_dist <- c(eu_dist, euclideanDist(test_data[i,], train_data[j,]))
#adding class variable of training data in eu_char
eu_char <- c(eu_char, as.character(train_data[j,'Y']))
}
eu <- data.frame(eu_char, eu_dist) #eu dataframe created with eu_char & eu_dist columns
eu <- eu[order(eu$eu_dist),] #sorting eu dataframe to gettop K neighbors
eu <- eu[1:k_value,] #eu dataframe with top K neighbors
#Loop 3: loops over eu and counts classes of neibhors.
for(k in c(1:nrow(eu))){
if(as.character(eu[k,"eu_char"]) == "0"){
good = good + 1
}
else bad = bad + 1
}
# Compares the no. of neighbors with class label good or bad
if(good > bad){ #if majority of neighbors are good then put "g" in pred vector
pred <- c(pred, "0")
}
else if(good < bad){
#if majority of neighbors are bad then put "b" in pred vector
pred <- c(pred, "1")
}
}
return(pred) #return pred vector
}
kNN(test.df, train.df, 3)
library('data.table')
knn.df <- fread("/Volumes/Nifty/Cloudstation/_Uni/Master/actual/FS2018/TA/03_Code/MachineLearningJerome/Data/Data_normalized.csv")
train.df <- knn.df
test.df <- data.frame("V1"="11", "Y"=0, "X1"=-0.55, "X2"=-0.80)
kNN(test.df, train.df, 3)
library('data.table')
knn.df <- fread("/Volumes/Nifty/Cloudstation/_Uni/Master/actual/FS2018/TA/03_Code/MachineLearningJerome/Data/Data_normalized.csv")
train.df <- knn.df
test.df <- data.frame("V1"="11", "Y"=0, "X1"=-0.55, "X2"=-0.80)
kNN(test.df, train.df, 3)
library('data.table')
knn.df <- fread("/Volumes/Nifty/Cloudstation/_Uni/Master/actual/FS2018/TA/03_Code/MachineLearningJerome/Data/Data_normalized.csv")
train.df <- knn.df
test.df <- data.frame("V1"="11", "Y"=0, "X1"=-0.55, "X2"=-0.80)
kNN(test.df, train.df, 3)
library('data.table')
knn.df <- fread("/Volumes/Nifty/Cloudstation/_Uni/Master/actual/FS2018/TA/03_Code/MachineLearningJerome/Data/Data_normalized.csv")
train.df <- knn.df
test.df <- data.frame("V1"="11", "Y"=0, "X1"=-0.55, "X2"=-0.80)
kNN(test.df, train.df, 3)
euclideanDist <- function(a, b){
d = 0
for (i in c(3:(length(a))))
{
d = d + (a[[i]]-b[[i]])^2
}
d = sqrt(d)
return(d)
}
for(i in c(1:nrow(test_data))){ #looping over each record of test data
eu_dist = c() #eu_dist & eu_char empty vector
eu_char = c()
good = 0 #good & bad variable initialization with 0 value
bad = 0
#LOOP-2-looping over train data
for(j in c(1:nrow(train_data))){
#adding euclidean distance b/w test data point and train data to eu_dist vector
eu_dist <- c(eu_dist, euclideanDist(test_data[i,], train_data[j,]))
#adding class variable of training data in eu_char
eu_char <- c(eu_char, as.character(train_data[j,'Y']))
}
eu <- data.frame(eu_char, eu_dist) #eu dataframe created with eu_char & eu_dist columns
eu <- eu[order(eu$eu_dist),] #sorting eu dataframe to gettop K neighbors
eu <- eu[1:k_value,] #eu dataframe with top K neighbors
#Loop 3: loops over eu and counts classes of neibhors.
for(k in c(1:nrow(eu))){
if(as.character(eu[k,"eu_char"]) == "0"){
good = good + 1
}
else bad = bad + 1
}
# Compares the no. of neighbors with class label good or bad
if(good > bad){ #if majority of neighbors are good then put "g" in pred vector
pred <- c(pred, "0")
}
else if(good < bad){
#if majority of neighbors are bad then put "b" in pred vector
pred <- c(pred, "1")
}
}
kNN <- function(test_data, train_data, k_value) {
pred <- c() #empty pred vector
#LOOP-1
for(i in c(1:nrow(test_data))){ #looping over each record of test data
eu_dist = c() #eu_dist & eu_char empty vector
eu_char = c()
good = 0 #good & bad variable initialization with 0 value
bad = 0
#LOOP-2-looping over train data
for(j in c(1:nrow(train_data))){
#adding euclidean distance b/w test data point and train data to eu_dist vector
eu_dist <- c(eu_dist, euclideanDist(test_data[i,], train_data[j,]))
#adding class variable of training data in eu_char
eu_char <- c(eu_char, as.character(train_data[j,'Y']))
}
eu <- data.frame(eu_char, eu_dist) #eu dataframe created with eu_char & eu_dist columns
eu <- eu[order(eu$eu_dist),] #sorting eu dataframe to gettop K neighbors
eu <- eu[1:k_value,] #eu dataframe with top K neighbors
#Loop 3: loops over eu and counts classes of neibhors.
for(k in c(1:nrow(eu))){
if(as.character(eu[k,"eu_char"]) == "0"){
good = good + 1
}
else bad = bad + 1
}
# Compares the no. of neighbors with class label good or bad
if(good > bad){ #if majority of neighbors are good then put "g" in pred vector
pred <- c(pred, "0")
}
else if(good < bad){
#if majority of neighbors are bad then put "b" in pred vector
pred <- c(pred, "1")
}
}
return(pred) #return pred vector
}
library('data.table')
knn.df <- fread("/Volumes/Nifty/Cloudstation/_Uni/Master/actual/FS2018/TA/03_Code/MachineLearningJerome/Data/Data_normalized.csv")
train.df <- knn.df
test.df <- data.frame("V1"="11", "Y"=0, "X1"=-0.55, "X2"=-0.80)
kNN(test.df, train.df, 3)
kNN <- function(test_data, train_data, k_value) {
pred <- c() #empty pred vector
#LOOP-1
for(i in c(1:nrow(test_data))){ #looping over each record of test data
eu_dist = c() #eu_dist & eu_char empty vector
eu_char = c()
class0 = 0 #class0 & class1 variable initialization with 0 value
class1 = 0
#LOOP-2-looping over train data
for(j in c(1:nrow(train_data))){
#adding euclidean distance b/w test data point and train data to eu_dist vector
eu_dist <- c(eu_dist, euclideanDist(test_data[i,], train_data[j,]))
#adding class variable of training data in eu_char
eu_char <- c(eu_char, as.character(train_data[j,'Y']))
}
eu <- data.frame(eu_char, eu_dist) #eu dataframe created with eu_char & eu_dist columns
eu <- eu[order(eu$eu_dist),] #sorting eu dataframe to get top K neighbors
eu <- eu[1:k_value,] #eu dataframe with top K neighbors
#Loop 3: loops over eu and counts classes of neibhors.
for(k in c(1:nrow(eu))){
if(as.character(eu[k,"eu_char"]) == "0"){
class0 = class0 + 1
}
else class1 = class1 + 1
}
# Compares the no. of neighbors with class label class0 or class1
if(class0 > class1){ #if majority of neighbors are class0 then put "0" in pred vector
pred <- c(pred, "0")
}
else if(class0 < class1){
#if majority of neighbors are class1 then put "1" in pred vector
pred <- c(pred, "1")
}
}
return(pred) #return pred vector
}
library('data.table')
knn.df <- fread("/Volumes/Nifty/Cloudstation/_Uni/Master/actual/FS2018/TA/03_Code/MachineLearningJerome/Data/Data_normalized.csv")
train.df <- knn.df
test.df <- data.frame("V1"="11", "Y"=0, "X1"=-0.55, "X2"=-0.80)
kNN(test.df, train.df, 5)
library('data.table')
knn.df <- fread("/Volumes/Nifty/Cloudstation/_Uni/Master/actual/FS2018/TA/03_Code/MachineLearningJerome/Data/Data_normalized.csv")
train.df <- knn.df
test.df <- data.frame("V1"="11", "Y"=0, "X1"=0.3, "X2"=0.3)
kNN(test.df, train.df, 5)
library('data.table')
knn.df <- fread("/Volumes/Nifty/Cloudstation/_Uni/Master/actual/FS2018/TA/03_Code/MachineLearningJerome/Data/Data_normalized.csv")
train.df <- knn.df
test.df <- data.frame("V1"="11", "Y"=0, "X1"=0, "X2"=0)
kNN(test.df, train.df, 5)
test.df <- data.frame("V1"="11", "Y"=0, "X1"=-0.1, "X2"=-0.1)
kNN(test.df, train.df, 5)
library('data.table')
knn.df <- fread("/Volumes/Nifty/Cloudstation/_Uni/Master/actual/FS2018/TA/03_Code/MachineLearningJerome/Data/Data_normalized.csv")
train.df <- knn.df
test.df <- data.frame("X1"=-0.55, "X2"=-0.80)
kNN(test.df, train.df, 5)
library('data.table')
knn.df <- fread("/Volumes/Nifty/Cloudstation/_Uni/Master/actual/FS2018/TA/03_Code/MachineLearningJerome/Data/Data_normalized.csv")
train.df <- knn.df
test.df <- data.frame("V1"="11", "Y"="?", "X1"=-0.55, "X2"=-0.80)
kNN(test.df, train.df, 5)
library('data.table')
knn.df <- fread("/Volumes/Nifty/Cloudstation/_Uni/Master/actual/FS2018/TA/03_Code/MachineLearningJerome/Data/Data_normalized.csv")
train.df <- knn.df
test.df <- data.frame("V1"="?", "Y"="?", "X1"=-0.55, "X2"=-0.80)
pred <- kNN(test.df, train.df, 5)
install.packages('data.table')
library('data.table')
knn.df <- fread("/Volumes/Nifty/Cloudstation/_Uni/Master/actual/FS2018/TA/03_Code/MachineLearningJerome/Data/Data_normalized.csv")
train.df <- knn.df
test.df <- data.frame("V1"="?", "Y"="?", "X1"=-0.55, "X2"=-0.80)
pred <- kNN(test.df, train.df, 5)
euclideanDist <- function(a, b){
d = 0
for (i in c(3:(length(a)))) #this for-loop covers columns 3 and 4 (--> X1, X2 coordinates)
{
d = d + (a[[i]]-b[[i]])^2
}
d = sqrt(d)
return(d)
}
kNN <- function(test_data, train_data, k_value) {
pred <- c() #empty pred vector
#LOOP-1
for(i in c(1:nrow(test_data))){ #looping over each record of test data
eu_dist = c() #eu_dist & eu_char empty vector
eu_char = c()
class0 = 0 #class0 & class1 variable initialization with 0 value
class1 = 0
#LOOP-2-looping over train data
for(j in c(1:nrow(train_data))){
#adding euclidean distance b/w test data point and train data to eu_dist vector
eu_dist <- c(eu_dist, euclideanDist(test_data[i,], train_data[j,]))
#adding class variable of training data in eu_char
eu_char <- c(eu_char, as.character(train_data[j,'Y']))
}
eu <- data.frame(eu_char, eu_dist) #eu dataframe created with eu_char & eu_dist columns
eu <- eu[order(eu$eu_dist),] #sorting eu dataframe to get top K neighbors
eu <- eu[1:k_value,] #eu dataframe with top K neighbors
#Loop 3: loops over eu and counts classes of neibhors.
for(k in c(1:nrow(eu))){
if(as.character(eu[k,"eu_char"]) == "0"){
class0 = class0 + 1
}
else class1 = class1 + 1
}
# Compares the no. of neighbors with class label class0 or class1
if(class0 > class1){ #if majority of neighbors are class0 then put "0" in pred vector
pred <- c(pred, "0")
}
else if(class0 < class1){
#if majority of neighbors are class1 then put "1" in pred vector
pred <- c(pred, "1")
}
}
return(pred) #return pred vector
}
install.packages("data.table")
install.packages('data.table')
library('data.table')
knn.df <- fread("/Volumes/Nifty/Cloudstation/_Uni/Master/actual/FS2018/TA/03_Code/MachineLearningJerome/Data/Data_normalized.csv")
train.df <- knn.df
test.df <- data.table("V1"="?", "Y"="?", "X1"=-0.55, "X2"=-0.80)
prediction <- kNN(test.df, train.df, 5)
#kNN Machine Learning Algorithm, classifying every row of an input data.frame (test_data) based on a data.table of already available points (train_data)
kNN <- function(test_data, train_data, k_value) {
#Create empty prediction vector for the results
predictionOutput <- c()
#Outer loop that iterates over the rows in test_data (the data to classify)
for(i in c(1:nrow(test_data))) {
#Creating empty vectors for intermediary results
eu_dist = c() #eu_dist & eu_char empty vector
eu_char = c()
#Variables to store the sum of the k-nearest classifiers
class0 = 0
class1 = 0
#Step 1 (Slide: 65) Compute the distance from the new point(s) in the data.table test_data to all other points (train_data).
for(j in c(1:nrow(train_data))){
#Calculating the euclidean distance between the to-classify point and all already available points.
eu_dist <- c(eu_dist, euclideanDist(test_data[i,], train_data[j,]))
#Creating a vector from the existing data converting the 0, 1 values into characters.
eu_char <- c(eu_char, as.character(train_data[j,'Y']))
}
#Step 2: (Slide: 76) Choose the number of neighbors.
#Merge the eu_char & eu_dist columns into one data.table.
eu <- data.table(eu_char, eu_dist)
#Sorting eu dataframe to get top K neighbors.
eu <- eu[order(eu$eu_dist),]
#Extracting top k neighbors.
eu <- eu[1:k_value,]
#Step 3 (Slide 77): Record the class labels of the k nearest neighbors.
for(k in c(1:nrow(eu))){
if(as.character(eu[k,"eu_char"]) == "0"){
class0 = class0 + 1
}
else class1 = class1 + 1
}
#Step 4: (Slide: 82) Majority voting. Compares the number of neighbors with class label class0 or class1.
#If the majority of neighbors are class0, then the output vector "predictionOutput" will contain a 0 at this position.
if(class0 > class1) {
predictionOutput <- c(predictionOutput, "0")
}
else if(class0 < class1){
#If the majority of neighbors are class1, then the output vector "predictionOutput" will contain a 1 at this position.
predictionOutput <- c(predictionOutput, "1")
}
}
#Returning the predictionOutput vector.
return(predictionOutput)
}
euclideanDist <- function(a, b){
d = 0
for (i in c(3:(length(a)))) #this for-loop covers columns 3 and 4 (--> X1, X2 coordinates)
{
d = d + (a[[i]]-b[[i]])^2
}
d = sqrt(d)
return(d)
}
library('data.table')
knn.df <- fread("Data_normalized.csv")
train.df <- knn.df
test.df <- data.table("V1"="?", "Y"="?", "X1"=-0.55, "X2"=-0.80)
prediction <- kNN(test.df, train.df, 5)
knn.df <- data.frame(index = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10"),
Y = c(0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 0L),
X1 = c(-0.968746351362512, -0.686929230966145, -1.14488205161024, -0.933519211312966, 0.264203550371594, 1.14488205161024, 0.757383511065237, 1.32101775185797, -0.792610651114783, 1.0392006314616),
X2 = c(-0.802661751617162, -0.974660698392268, -1.08932666290901, -1.08932666290901, 0.630662804842056, 1.08932666290901, 0.9173277161339, 1.49065753871759, 0.515996840325318, 0.343997893550212)
)
train.df <- knn.df
#V1 and Y are used as placeholders, they are not required by the algorithm.
test.df <- data.table("index"="?", "Y"="?", "X1"=-0.55, "X2"=-0.80)
knn.df <- data.frame(index = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10"),
Y = c(0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 0L),
X1 = c(-0.968746351362512, -0.686929230966145, -1.14488205161024, -0.933519211312966, 0.264203550371594, 1.14488205161024, 0.757383511065237, 1.32101775185797, -0.792610651114783, 1.0392006314616),
X2 = c(-0.802661751617162, -0.974660698392268, -1.08932666290901, -1.08932666290901, 0.630662804842056, 1.08932666290901, 0.9173277161339, 1.49065753871759, 0.515996840325318, 0.343997893550212)
)
train.df <- knn.df
#V1 and Y are used as placeholders, they are not required by the algorithm.
test.df <- data.frame("index"="?", "Y"="?", "X1"=-0.55, "X2"=-0.80)
prediction <- kNN(test.df, train.df, 5)
#kNN Machine Learning Algorithm, classifying every row of an input data.frame (test_data) based on a data.table of already available points (train_data)
kNN <- function(test_data, train_data, k_value) {
#Create empty prediction vector for the results
predictionOutput <- c()
#Outer loop that iterates over the rows in test_data (the data to classify)
for(i in c(1:nrow(test_data))) {
#Creating empty vectors for intermediary results
eu_dist = c() #eu_dist & eu_char empty vector
eu_char = c()
#Variables to store the sum of the k-nearest classifiers
class0 = 0
class1 = 0
#Step 1 (Slide: 65) Compute the distance from the new point(s) in the data.table test_data to all other points (train_data).
for(j in c(1:nrow(train_data))){
#Calculating the euclidean distance between the to-classify point and all already available points.
eu_dist <- c(eu_dist, euclideanDist(test_data[i,], train_data[j,]))
#Creating a vector from the existing data converting the 0, 1 values into characters.
eu_char <- c(eu_char, as.character(train_data[j,'Y']))
}
#Step 2: (Slide: 76) Choose the number of neighbors.
#Merge the eu_char & eu_dist columns into one data.table.
eu <- data.frame(eu_char, eu_dist)
#Sorting eu dataframe to get top K neighbors.
eu <- eu[order(eu$eu_dist),]
#Extracting top k neighbors.
eu <- eu[1:k_value,]
#Step 3 (Slide 77): Record the class labels of the k nearest neighbors.
for(k in c(1:nrow(eu))){
if(as.character(eu[k,"eu_char"]) == "0"){
class0 = class0 + 1
}
else class1 = class1 + 1
}
#Step 4: (Slide: 82) Majority voting. Compares the number of neighbors with class label class0 or class1.
#If the majority of neighbors are class0, then the output vector "predictionOutput" will contain a 0 at this position.
if(class0 > class1) {
predictionOutput <- c(predictionOutput, "0")
}
else if(class0 < class1){
#If the majority of neighbors are class1, then the output vector "predictionOutput" will contain a 1 at this position.
predictionOutput <- c(predictionOutput, "1")
}
}
#Returning the predictionOutput vector.
return(predictionOutput)
}
euclideanDist <- function(a, b){
d = 0
#Iterating over the X1 and X2 coordinates (positions 3 and 4 in the data.table)
for (i in c(3:(length(a))))
{
d = d + (a[[i]]-b[[i]])^2
}
d = sqrt(d)
return(d)
}
knn.df <- data.frame(index = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10"),
Y = c(0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 0L),
X1 = c(-0.968746351362512, -0.686929230966145, -1.14488205161024, -0.933519211312966, 0.264203550371594, 1.14488205161024, 0.757383511065237, 1.32101775185797, -0.792610651114783, 1.0392006314616),
X2 = c(-0.802661751617162, -0.974660698392268, -1.08932666290901, -1.08932666290901, 0.630662804842056, 1.08932666290901, 0.9173277161339, 1.49065753871759, 0.515996840325318, 0.343997893550212)
)
train.df <- knn.df
#V1 and Y are used as placeholders, they are not required by the algorithm.
test.df <- data.frame("index"="?", "Y"="?", "X1"=-0.55, "X2"=-0.80)
prediction <- kNN(test.df, train.df, 5)
#kNN Machine Learning Algorithm, classifying every row of an input data.frame (test_data) based on a data.table of already available points (train_data)
kNN <- function(test_data, train_data, k_value) {
#Create empty prediction vector for the results
predictionOutput <- c()
#Outer loop that iterates over the rows in test_data (the data to classify)
for(i in c(1:nrow(test_data))) {
#Creating empty vectors for intermediary results
eu_dist = c() #eu_dist & eu_char empty vector
eu_char = c()
#Variables to store the sum of the k-nearest classifiers
class0 = 0
class1 = 0
#Step 1 (Slide: 65): Compute the distance from the new point(s) in the data.table test_data to all other points (train_data).
for(j in c(1:nrow(train_data))){
#Calculating the euclidean distance between the to-classify point and all already available points.
eu_dist <- c(eu_dist, euclideanDist(test_data[i,], train_data[j,]))
#Creating a vector from the existing data converting the 0, 1 values into characters.
eu_char <- c(eu_char, as.character(train_data[j,'Y']))
}
#Step 2: (Slide: 76): Choose the number of neighbors.
#Merge the eu_char & eu_dist columns into one data.table.
eu <- data.frame(eu_char, eu_dist)
#Sorting eu dataframe to get top K neighbors.
eu <- eu[order(eu$eu_dist),]
#Extracting top k neighbors.
eu <- eu[1:k_value,]
#Step 3 (Slide 77): Record the class labels of the k nearest neighbors.
for(k in c(1:nrow(eu))){
if(as.character(eu[k,"eu_char"]) == "0"){
class0 = class0 + 1
}
else class1 = class1 + 1
}
#Step 4: (Slide: 82) Majority voting. Compares the number of neighbors with class label class0 or class1.
#If the majority of neighbors are class0, then the output vector "predictionOutput" will contain a 0 at this position.
if(class0 > class1) {
predictionOutput <- c(predictionOutput, "0")
}
else if(class0 < class1){
#If the majority of neighbors are class1, then the output vector "predictionOutput" will contain a 1 at this position.
predictionOutput <- c(predictionOutput, "1")
}
}
#Returning the predictionOutput vector.
return(predictionOutput)
}
euclideanDist <- function(a, b){
d = 0
#Iterating over the X1 and X2 coordinates (positions 3 and 4 in the data.table)
for (i in c(3:(length(a))))
{
d = d + (a[[i]]-b[[i]])^2
}
d = sqrt(d)
return(d)
}
knn.df <- data.frame(index = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10"),
Y = c(0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 0L),
X1 = c(-0.968746351362512, -0.686929230966145, -1.14488205161024, -0.933519211312966, 0.264203550371594, 1.14488205161024, 0.757383511065237, 1.32101775185797, -0.792610651114783, 1.0392006314616),
X2 = c(-0.802661751617162, -0.974660698392268, -1.08932666290901, -1.08932666290901, 0.630662804842056, 1.08932666290901, 0.9173277161339, 1.49065753871759, 0.515996840325318, 0.343997893550212)
)
train.df <- knn.df
#V1 and Y are used as placeholders, they are not required by the algorithm.
test.df <- data.frame("index"="?", "Y"="?", "X1"=-0.55, "X2"=-0.80)
prediction <- kNN(test.df, train.df, 5)
trainData <- data.frame(index = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10"),
Y = c(0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 0L),
X1 = c(2, 2.8, 1.5, 2.1, 5.5, 8, 6.9, 8.5, 2.5, 7.7),
X2 = c(1.5, 1.2, 1, 1, 4, 4.8, 4.5, 5.5, 2, 3.5))
trainData <- scale(trainData)
trainData <- data.frame(index = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10"),
Y = c(0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 0L),
X1 = c(2, 2.8, 1.5, 2.1, 5.5, 8, 6.9, 8.5, 2.5, 7.7),
X2 = c(1.5, 1.2, 1, 1, 4, 4.8, 4.5, 5.5, 2, 3.5))
trainData <- scale(trainData)
trainData <- scale(trainData[,3:4])
View(trainData)
trainData[,3:4] <- scale(trainData[,3:4])
trainData <- data.frame(index = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10"),
Y = c(0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 0L),
X1 = c(2, 2.8, 1.5, 2.1, 5.5, 8, 6.9, 8.5, 2.5, 7.7),
X2 = c(1.5, 1.2, 1, 1, 4, 4.8, 4.5, 5.5, 2, 3.5))
trainData[,3:4] <- scale(trainData[,3:4])
View(trainData)
train_data <- data.frame(index = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10"),
Y = c(0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 0L),
X1 = c(2, 2.8, 1.5, 2.1, 5.5, 8, 6.9, 8.5, 2.5, 7.7),
X2 = c(1.5, 1.2, 1, 1, 4, 4.8, 4.5, 5.5, 2, 3.5))
train_data <- data.frame(index = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10"),
Y = c(0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 0L),
X1 = c(2, 2.8, 1.5, 2.1, 5.5, 8, 6.9, 8.5, 2.5, 7.7),
X2 = c(1.5, 1.2, 1, 1, 4, 4.8, 4.5, 5.5, 2, 3.5))
train_data[,3:4] <- scale(train_data[,3:4])
#V1 and Y are used as placeholders, they are not required by the algorithm.
test_data <- data.frame("index"="?", "Y"="?", "X1"=-0.55, "X2"=-0.80)
prediction <- kNN(test_data, train_data, 5)
train_data <- data.frame(index = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10"),
Y = c(0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 0L),
X1 = c(2, 2.8, 1.5, 2.1, 5.5, 8, 6.9, 8.5, 2.5, 7.7),
X2 = c(1.5, 1.2, 1, 1, 4, 4.8, 4.5, 5.5, 2, 3.5))
train_data[,3:4] <- scale(train_data[,3:4])
#V1 and Y are used as placeholders, they are not required by the algorithm.
test_data <- data.frame("index"="?", "Y"="?", "X1"=-0.55, "X2"=-0.80)
prediction <- kNN(test_data, train_data, 5)
prediction
