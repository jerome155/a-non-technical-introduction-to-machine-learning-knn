{
    "collab_server" : "",
    "contents" : "---\ntitle: \"kNN\"\noutput: html_document\n---\n\nkNN is the easiest classification algorithm as it has no explicit training\nphase, i.e. no model has to be learned and the class membership of new \nobservation can immediately be predicted given 2 model specifications of kNN:\n\n  * A distance measure, i.e. how to measure closeness of observations.\n  * A measure specifying which information to consider for the classification \n  decision, i.e. how many neighbors.\n  \nFour simple steps:\n\n1. Compute the distance from the new point to all the other training points.\n2. Choose the number of neighbors for classification.\n3. Record the class labels of the chosen neighbors.\n4. Majority voting among the class labels of the k nearest neighbors to \nclassify the new point.\n\nkNN-Method calculating the nearest neighbour for the test_data set:\n\n```{r}\n#kNN Machine Learning Algorithm, classifying every row of an input data.frame \n#(test_data) based on a data.frame of already available points (train_data)\nkNN <- function(test_data, train_data, k_value) {\n  #Create empty prediction vector for the results\n  predictionOutput <- c()\n  \n  #Outer loop that iterates over the rows in test_data (the data to classify)\n  for(i in c(1:nrow(test_data))) { \n    #Creating empty vectors for intermediary results\n    eu_dist = c() #eu_dist & eu_char empty vector\n    eu_char = c()\n    #Variables to store the sum of the k-nearest classifiers\n    class0 = 0\n    class1 = 0\n    \n    #Step 1 (Slide: 65): Compute the distance from the new point(s) in the \n    #data.frame test_data to all other points (train_data).\n    for(j in c(1:nrow(train_data))){\n      #Calculating the euclidean distance between the to-classify point and \n      #all already available points.\n      eu_dist <- c(eu_dist, euclideanDist(test_data[i,], train_data[j,]))\n      #Creating a vector from the existing data converting the 0, 1 values into\n      #characters.\n      eu_char <- c(eu_char, as.character(train_data[j,'Y'])) \n    }\n    \n    #Step 2: (Slide: 76): Choose the number of neighbors.\n    #Merge the eu_char & eu_dist columns into one data.frame.\n    eu <- data.frame(eu_char, eu_dist)\n    #Sorting eu dataframe to get top K neighbors.\n    eu <- eu[order(eu$eu_dist),]\n    #Extracting top k neighbors.\n    eu <- eu[1:k_value,] \n    \n    #Step 3 (Slide 77): Record the class labels of the k nearest neighbors.\n    for(k in c(1:nrow(eu))){ \n      if(as.character(eu[k,\"eu_char\"]) == \"0\"){\n        class0 = class0 + 1 \n      }\n      else class1 = class1 + 1 \n    }\n    \n    #Step 4: (Slide: 82) Majority voting. Compares the number of neighbors with\n    #class label class0 or class1.\n    #If the majority of neighbors are class0, then the output vector \n    #\"predictionOutput\" will contain a 0 at this position.\n    if(class0 > class1) {\n      predictionOutput <- c(predictionOutput, \"0\") \n    }\n    else if(class0 < class1){\n      #If the majority of neighbors are class1, then the output vector \n      #\"predictionOutput\" will contain a 1 at this position.\n      predictionOutput <- c(predictionOutput, \"1\") \n    }\n  }\n  #Returning the predictionOutput vector.\n  return(predictionOutput)\n}\n```\n\nFunction that calculates the Euclidean distance:\n\n```{r}\n\neuclideanDist <- function(a, b){\n  d = 0\n  #Iterating over the X1 and X2 coordinates (positions 3 and 4 in the \n  #data.frame)\n  for (i in c(3:(length(a)))) \n  {\n    d = d + (a[[i]]-b[[i]])^2\n  }\n  d = sqrt(d)\n  return(d)\n}\n```\n\nGetting data into R and starting the prediction process:\n\n```{r}\ntrain_data <- data.frame(index = c(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"),\n                        Y = c(0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 0L),\n                        X1 = c(2, 2.8, 1.5, 2.1, 5.5, 8, 6.9, 8.5, 2.5, 7.7),\n                        X2 = c(1.5, 1.2, 1, 1, 4, 4.8, 4.5, 5.5, 2, 3.5))\ntrain_data[,3:4] <- scale(train_data[,3:4])\n\n#V1 and Y are used as placeholders, they are not required by the algorithm.\ntest_data <- data.frame(\"index\"=\"?\", \"Y\"=\"?\", \"X1\"=-0.55, \"X2\"=-0.80)\n\nprediction <- kNN(test_data, train_data, 5)\nprediction\n```\n",
    "created" : 1520325683727.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "807356540",
    "id" : "E2235DDE",
    "lastKnownWriteTime" : 1531294995,
    "last_content_update" : 1531294995561,
    "path" : "/Volumes/Nifty/Cloudstation/_Uni/Master/actual/FS2018/TA/02_R/01_KNN/KNN.Rmd",
    "project_path" : "KNN.Rmd",
    "properties" : {
        "chunk_output_type" : "inline",
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}